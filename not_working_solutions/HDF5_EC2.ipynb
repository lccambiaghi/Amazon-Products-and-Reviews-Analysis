{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First example with HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import ujson\n",
    "import gzip\n",
    "import ast\n",
    "from pandas import DataFrame\n",
    "from toolz import dissoc\n",
    "from toolz import dissoc, partition_all\n",
    "from castra import Castra\n",
    "import time\n",
    "import datetime\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "from dask.diagnostics import ProgressBar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import HDFStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Creating with h5py and reading with Dask: conflict between h5py and Dask?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_to_file = \"/home/ec2-user/amazon_dataset/data.hdf5\"\n",
    "df = pd.DataFrame(np.random.randn(10, 5))\n",
    "current_index = 10 #Starting from 0, pointing to the first \"free\" position\n",
    "chunks = 10 #writing 10 lines each time\n",
    "\n",
    "with h5py.File(path_to_file, \"w\") as hf:\n",
    "    hf.create_dataset('amazon', data=df,maxshape=(None,None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of arrays in this file: \n",
      "[u'amazon']\n",
      "Shape of the array amazon: \n",
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path_to_file, \"r\") as hf:\n",
    "    print 'List of arrays in this file: \\n', hf.keys()\n",
    "    data = hf.get('amazon')\n",
    "    np_data = np.array(data)\n",
    "    print'Shape of the array amazon: \\n', np_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to reshape and add more content to the HDF5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of arrays in this file: \n",
      "[u'amazon']\n",
      "Hierarchy: /amazon\n",
      "Shape of the array amazon: \n",
      "(20, 5)\n",
      "Data:\n",
      "[[ 0.33615614  0.92148885  0.80791913 -0.07501982  1.50430268]\n",
      " [ 0.7479168   0.65582071 -0.71301003  0.9000312   2.12067986]\n",
      " [ 0.28266367 -0.93705573 -0.93001321 -0.04424039  0.37498966]\n",
      " [ 0.69727914  0.88918464 -2.12194092  1.03283473 -0.23717406]\n",
      " [-0.22126272 -0.18595924  0.70276079 -0.25828817  0.03022614]\n",
      " [-0.25548966 -0.50407185 -0.68975554  0.67881699  0.375861  ]\n",
      " [ 1.68238172 -0.94102896 -0.91619115 -0.22347315 -0.48222473]\n",
      " [ 0.44037292  0.1177223   0.73702812 -0.71398744  1.0187634 ]\n",
      " [-0.99370112 -0.366224   -0.56279895  0.26389202 -0.59253603]\n",
      " [-0.71683573  1.2921071  -1.59845757  0.719606   -0.8612614 ]\n",
      " [ 0.72463843  0.07863037  0.00382157 -1.39810272 -1.81331261]\n",
      " [-0.45035738  0.7919192  -0.40131715  0.12347294 -0.81692247]\n",
      " [ 0.07909288  1.13137146  1.43555496 -1.92644076  1.05954108]\n",
      " [ 0.68715044 -0.55307139 -1.49701966 -1.61766041  0.59391732]\n",
      " [-2.79255144 -0.8156708   0.14451914  0.55447627 -0.69890438]\n",
      " [ 1.15234974 -1.39836278 -0.69343631 -0.70537229 -1.03612067]\n",
      " [-0.29628214 -0.20714572 -1.00386495 -3.06484895  0.77176389]\n",
      " [ 0.41928378  0.27947715 -0.53829255  0.39573861  1.95734273]\n",
      " [-1.12169001 -0.17934515 -1.15341126 -0.05267281  0.47773748]\n",
      " [-0.38232652  0.86209742  0.31018013 -1.11466294  0.08811476]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path_to_file, \"r+\") as hf:\n",
    "    data = hf.get('amazon')\n",
    "    df = pd.DataFrame(np.random.randn(10, 5)) #New dataframe to add\n",
    "    data.resize((data.shape[0]+chunks,data.shape[1]))\n",
    "    data[current_index:current_index+chunks] = df\n",
    "    print 'List of arrays in this file: \\n', hf.keys()\n",
    "    print 'Hierarchy:',data.name\n",
    "    np_data = np.array(data)\n",
    "    print'Shape of the array amazon: \\n', np_data.shape\n",
    "    print 'Data:\\n',data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input must be a list longer than 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-126cacf47380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/amazon'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/dataframe/io/hdf.pyc\u001b[0m in \u001b[0;36mread_hdf\u001b[0;34m(pattern, key, start, stop, columns, chunksize, sorted_index, lock, mode)\u001b[0m\n\u001b[1;32m    316\u001b[0m                                     \u001b[0msorted_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorted_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                                     lock=lock, mode=mode)\n\u001b[0;32m--> 318\u001b[0;31m                    for path in paths])\n\u001b[0m",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/dataframe/io/hdf.pyc\u001b[0m in \u001b[0;36m_read_single_hdf\u001b[0;34m(path, key, start, stop, columns, chunksize, sorted_index, lock, mode)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     return concat([one_path_one_key(path, k, start, s, columns, chunksize, d, lock)\n\u001b[0;32m--> 248\u001b[0;31m                    for k, s, d in zip(keys, stops, divisions)])\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/dataframe/multi.pyc\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(dfs, axis, join, interleave_partitions)\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input must be a list longer than 0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input must be a list longer than 0"
     ]
    }
   ],
   "source": [
    "dd.read_hdf(path_to_file,'/amazon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Creating HDF5 with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,)\n",
      "[ (0, [1.4563122393852164, -0.24982073624775278, -0.3293450140017712, -2.772855602530441, -0.058142051682005445])\n",
      " (1, [0.7574309530244978, 0.24523064187019356, 1.410097766976578, -1.3332376792617133, -0.08484400847375759])\n",
      " (2, [0.38521326209434337, -0.7607452854170671, -1.138418831321935, -0.6150874524000192, 0.8428532223471128])\n",
      " (3, [0.721336005389751, -1.1442249074591915, 1.1684689945463134, -2.16217405435812, -1.004274068492681])\n",
      " (4, [0.6050078944205708, -0.6643434801552311, -1.762136984961981, 0.7668257407776377, -1.2093949801044968])\n",
      " (5, [0.15287912986610439, -0.8987083600616191, 0.5651140207231614, -0.43412461537983404, 0.12622315269492265])\n",
      " (6, [0.4849829072057437, -0.30284917884973395, 0.08285398467388008, 0.24550263975627878, 1.9310003355850383])\n",
      " (7, [0.14906799301316268, -0.40163177754128215, 0.8621989270922575, -1.29392807633283, -1.146961931116345])\n",
      " (8, [-0.3692064759819252, 0.6833280151980384, -0.27567742013271074, -0.20314684750770887, -0.3284001586766101])\n",
      " (9, [-0.5718480982176296, -0.977000857428816, -0.04656522794932702, 1.113768353290241, 1.9125244490910867])\n",
      " (0, [-0.875635676965116, 0.4575032667727679, -0.20234729321498124, 1.2416619945262577, 0.3000870586541005])\n",
      " (1, [-1.5693893372723833, -1.1760192218577514, 0.7058405672234899, 0.9184676367802588, -1.7490152801407701])\n",
      " (2, [0.6290257591828163, 0.5138723320921975, 3.1361882282794387, -1.5618633762255902, 0.02428022828262571])\n",
      " (3, [1.261181822079854, -1.328141012416987, 2.59209128760775, 0.22691575166050645, -0.5900137385899648])\n",
      " (4, [-0.08963353809409935, 1.7784026528925634, 1.2088324040060017, 0.4381244490782096, -1.9746162931304245])\n",
      " (5, [0.08874586021652923, 1.016481320933456, -2.0297189484859355, 0.8974423824511361, 0.0705164398991296])\n",
      " (6, [0.4301432776178793, -0.19875971322352265, -0.6802774758435067, 0.9146467149143387, -0.07133926286232005])\n",
      " (7, [0.86261225771058, 0.4664377756556633, -0.7976944296542976, -0.9985398206551406, -1.000121062347002])\n",
      " (8, [1.894508269484697, -1.105479053975507, 0.21411261807990253, 2.4799606154245484, 0.23468280505437072])\n",
      " (9, [-1.357282713812901, -1.6000793333256542, 0.8035246288622192, 0.8068564287324256, 2.5072920124310167])\n",
      " (0, [-0.518120651807614, -2.159603650536196, 0.7307475914264722, -0.9479835777968362, 0.2527193457904102])\n",
      " (1, [1.270259143051351, 0.27195368639924705, -1.7793439221574516, -0.5828248493928894, 0.26561954548605643])\n",
      " (2, [-0.17235123721325724, 1.5286169116063177, -1.5840071752487708, 0.2764412542423264, 0.9780802491573598])\n",
      " (3, [-1.0120257946012698, -0.6253341288082411, -0.09299974652776499, 1.9766574003685304, -1.715494536013931])\n",
      " (4, [2.0883237345587014, -0.3628360116739487, 1.1926072493655757, 0.08999938958006888, 1.5767645520461453])\n",
      " (5, [-0.22884488903204672, -0.7473285260963602, 2.299129787530035, 0.46771242690500997, -0.04099378465981984])\n",
      " (6, [-0.23306115344296582, -0.3502005860399337, 2.0829744359931115, -2.1750902602088296, -0.4049774033075053])\n",
      " (7, [-0.6765004198625549, -0.5136437772390449, -0.8789674908893336, 0.2680543243228751, 0.9772635774587816])\n",
      " (8, [-1.5949105849502372, 0.14782441352632492, -0.3260680266110463, 0.6457312627315158, -0.8174506506167883])\n",
      " (9, [0.04402059907181241, 0.42163567585923895, 0.2829069929674399, -0.26284346913691137, 1.3395440633669133])]\n"
     ]
    }
   ],
   "source": [
    "#First DF\n",
    "df = pd.DataFrame(np.random.randn(10, 5))\n",
    "#Create HDF5\n",
    "df.to_hdf(path_to_file,'/amazon',format='table')\n",
    "\n",
    "#Second DF\n",
    "df = pd.DataFrame(np.random.randn(10, 5))\n",
    "df.to_hdf(path_to_file,'/amazon',format='table',append=True)\n",
    "\n",
    "#Third DF\n",
    "df = pd.DataFrame(np.random.randn(10, 5))\n",
    "df.to_hdf(path_to_file,'/amazon',format='table',append=True)\n",
    "\n",
    "#Check status\n",
    "with h5py.File(path_to_file, \"r+\") as hf:\n",
    "    data = hf.get('amazon/table')\n",
    "    print data.shape\n",
    "    print data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with h5py.File(path_to_file, \"r+\") as hf:\\n    data = hf.get(\\'amazon/table\\')\\n    print data[:]\\n    df = pd.DataFrame(np.random.randn(10, 5))\\n    print data.shape\\n    data.resize((data.shape[0]+chunks,))\\n    print data.shape\\n    data[current_index:current_index+chunks] = df\\n    print \\'Data:\\n\\',data[:]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df = pd.DataFrame(np.random.randn(10, 5))\n",
    "df.to_hdf(path_to_file,'/amazon',format='table')'''\n",
    "ddf = dd.read_hdf(path_to_file,key='/amazon') #Dask can read correctly HDF5 created with Pandas\n",
    "'''with h5py.File(path_to_file, \"r+\") as hf:\n",
    "    data = hf.get('amazon/table')\n",
    "    print data[:]\n",
    "    df = pd.DataFrame(np.random.randn(10, 5))\n",
    "    print data.shape\n",
    "    data.resize((data.shape[0]+chunks,))\n",
    "    print data.shape\n",
    "    data[current_index:current_index+chunks] = df\n",
    "    print 'Data:\\n',data[:]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.456312</td>\n",
       "      <td>-0.249821</td>\n",
       "      <td>-0.329345</td>\n",
       "      <td>-2.772856</td>\n",
       "      <td>-0.058142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.757431</td>\n",
       "      <td>0.245231</td>\n",
       "      <td>1.410098</td>\n",
       "      <td>-1.333238</td>\n",
       "      <td>-0.084844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.385213</td>\n",
       "      <td>-0.760745</td>\n",
       "      <td>-1.138419</td>\n",
       "      <td>-0.615087</td>\n",
       "      <td>0.842853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.721336</td>\n",
       "      <td>-1.144225</td>\n",
       "      <td>1.168469</td>\n",
       "      <td>-2.162174</td>\n",
       "      <td>-1.004274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.605008</td>\n",
       "      <td>-0.664343</td>\n",
       "      <td>-1.762137</td>\n",
       "      <td>0.766826</td>\n",
       "      <td>-1.209395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.152879</td>\n",
       "      <td>-0.898708</td>\n",
       "      <td>0.565114</td>\n",
       "      <td>-0.434125</td>\n",
       "      <td>0.126223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.484983</td>\n",
       "      <td>-0.302849</td>\n",
       "      <td>0.082854</td>\n",
       "      <td>0.245503</td>\n",
       "      <td>1.931000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.149068</td>\n",
       "      <td>-0.401632</td>\n",
       "      <td>0.862199</td>\n",
       "      <td>-1.293928</td>\n",
       "      <td>-1.146962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.369206</td>\n",
       "      <td>0.683328</td>\n",
       "      <td>-0.275677</td>\n",
       "      <td>-0.203147</td>\n",
       "      <td>-0.328400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.571848</td>\n",
       "      <td>-0.977001</td>\n",
       "      <td>-0.046565</td>\n",
       "      <td>1.113768</td>\n",
       "      <td>1.912524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.875636</td>\n",
       "      <td>0.457503</td>\n",
       "      <td>-0.202347</td>\n",
       "      <td>1.241662</td>\n",
       "      <td>0.300087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.569389</td>\n",
       "      <td>-1.176019</td>\n",
       "      <td>0.705841</td>\n",
       "      <td>0.918468</td>\n",
       "      <td>-1.749015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.629026</td>\n",
       "      <td>0.513872</td>\n",
       "      <td>3.136188</td>\n",
       "      <td>-1.561863</td>\n",
       "      <td>0.024280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.261182</td>\n",
       "      <td>-1.328141</td>\n",
       "      <td>2.592091</td>\n",
       "      <td>0.226916</td>\n",
       "      <td>-0.590014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.089634</td>\n",
       "      <td>1.778403</td>\n",
       "      <td>1.208832</td>\n",
       "      <td>0.438124</td>\n",
       "      <td>-1.974616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.088746</td>\n",
       "      <td>1.016481</td>\n",
       "      <td>-2.029719</td>\n",
       "      <td>0.897442</td>\n",
       "      <td>0.070516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.430143</td>\n",
       "      <td>-0.198760</td>\n",
       "      <td>-0.680277</td>\n",
       "      <td>0.914647</td>\n",
       "      <td>-0.071339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.862612</td>\n",
       "      <td>0.466438</td>\n",
       "      <td>-0.797694</td>\n",
       "      <td>-0.998540</td>\n",
       "      <td>-1.000121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.894508</td>\n",
       "      <td>-1.105479</td>\n",
       "      <td>0.214113</td>\n",
       "      <td>2.479961</td>\n",
       "      <td>0.234683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.357283</td>\n",
       "      <td>-1.600079</td>\n",
       "      <td>0.803525</td>\n",
       "      <td>0.806856</td>\n",
       "      <td>2.507292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.518121</td>\n",
       "      <td>-2.159604</td>\n",
       "      <td>0.730748</td>\n",
       "      <td>-0.947984</td>\n",
       "      <td>0.252719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.270259</td>\n",
       "      <td>0.271954</td>\n",
       "      <td>-1.779344</td>\n",
       "      <td>-0.582825</td>\n",
       "      <td>0.265620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.172351</td>\n",
       "      <td>1.528617</td>\n",
       "      <td>-1.584007</td>\n",
       "      <td>0.276441</td>\n",
       "      <td>0.978080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.012026</td>\n",
       "      <td>-0.625334</td>\n",
       "      <td>-0.093000</td>\n",
       "      <td>1.976657</td>\n",
       "      <td>-1.715495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.088324</td>\n",
       "      <td>-0.362836</td>\n",
       "      <td>1.192607</td>\n",
       "      <td>0.089999</td>\n",
       "      <td>1.576765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.228845</td>\n",
       "      <td>-0.747329</td>\n",
       "      <td>2.299130</td>\n",
       "      <td>0.467712</td>\n",
       "      <td>-0.040994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.233061</td>\n",
       "      <td>-0.350201</td>\n",
       "      <td>2.082974</td>\n",
       "      <td>-2.175090</td>\n",
       "      <td>-0.404977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.676500</td>\n",
       "      <td>-0.513644</td>\n",
       "      <td>-0.878967</td>\n",
       "      <td>0.268054</td>\n",
       "      <td>0.977264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.594911</td>\n",
       "      <td>0.147824</td>\n",
       "      <td>-0.326068</td>\n",
       "      <td>0.645731</td>\n",
       "      <td>-0.817451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.044021</td>\n",
       "      <td>0.421636</td>\n",
       "      <td>0.282907</td>\n",
       "      <td>-0.262843</td>\n",
       "      <td>1.339544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4\n",
       "0  1.456312 -0.249821 -0.329345 -2.772856 -0.058142\n",
       "1  0.757431  0.245231  1.410098 -1.333238 -0.084844\n",
       "2  0.385213 -0.760745 -1.138419 -0.615087  0.842853\n",
       "3  0.721336 -1.144225  1.168469 -2.162174 -1.004274\n",
       "4  0.605008 -0.664343 -1.762137  0.766826 -1.209395\n",
       "5  0.152879 -0.898708  0.565114 -0.434125  0.126223\n",
       "6  0.484983 -0.302849  0.082854  0.245503  1.931000\n",
       "7  0.149068 -0.401632  0.862199 -1.293928 -1.146962\n",
       "8 -0.369206  0.683328 -0.275677 -0.203147 -0.328400\n",
       "9 -0.571848 -0.977001 -0.046565  1.113768  1.912524\n",
       "0 -0.875636  0.457503 -0.202347  1.241662  0.300087\n",
       "1 -1.569389 -1.176019  0.705841  0.918468 -1.749015\n",
       "2  0.629026  0.513872  3.136188 -1.561863  0.024280\n",
       "3  1.261182 -1.328141  2.592091  0.226916 -0.590014\n",
       "4 -0.089634  1.778403  1.208832  0.438124 -1.974616\n",
       "5  0.088746  1.016481 -2.029719  0.897442  0.070516\n",
       "6  0.430143 -0.198760 -0.680277  0.914647 -0.071339\n",
       "7  0.862612  0.466438 -0.797694 -0.998540 -1.000121\n",
       "8  1.894508 -1.105479  0.214113  2.479961  0.234683\n",
       "9 -1.357283 -1.600079  0.803525  0.806856  2.507292\n",
       "0 -0.518121 -2.159604  0.730748 -0.947984  0.252719\n",
       "1  1.270259  0.271954 -1.779344 -0.582825  0.265620\n",
       "2 -0.172351  1.528617 -1.584007  0.276441  0.978080\n",
       "3 -1.012026 -0.625334 -0.093000  1.976657 -1.715495\n",
       "4  2.088324 -0.362836  1.192607  0.089999  1.576765\n",
       "5 -0.228845 -0.747329  2.299130  0.467712 -0.040994\n",
       "6 -0.233061 -0.350201  2.082974 -2.175090 -0.404977\n",
       "7 -0.676500 -0.513644 -0.878967  0.268054  0.977264\n",
       "8 -1.594911  0.147824 -0.326068  0.645731 -0.817451\n",
       "9  0.044021  0.421636  0.282907 -0.262843  1.339544"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3) DataFrames with data from JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_to = \"/home/ec2-user/amazon_dataset/\"\n",
    "path_to_file = \"/home/ec2-user/amazon_dataset/data.hdf5\"\n",
    "f = 'reviews_Musical_Instruments_5.json'\n",
    "reviews_columns = ['asin', 'reviewerID','reviewerName', 'overall','summary','reviewText','reviewTime','unixReviewTime']\n",
    "metadata_columns = ['asin','title','price','imUrl','related','also_bought','also_viewed','bought_together','salesRank','brand','categories']\n",
    "str_dt = h5py.special_dtype(vlen=str)\n",
    "#dtypes = np.dtype([('asin',str_dt),('reviewerID',str_dt),('reviewerName',str_dt),('overall',float),('summary',str_dt),\n",
    "#                  ('reviewText',str_dt),('reviewTime',str_dt),('unixReviewTime',int)])\n",
    "chunksize = 5000\n",
    "current_index = 0\n",
    "\n",
    "#Convert a line of JSON into a cleaned up dict.\n",
    "def to_json(line):\n",
    "    return ujson.loads(line.encode('utf8'))\n",
    "\n",
    "#Convert a not proper line of JSON (due to single quotes) into a cleaned up dict.\n",
    "def fix_json(line):\n",
    "    return ast.literal_eval(line)\n",
    "\n",
    "#Convert a list of JSON strings into a DataFrame\n",
    "def to_df(batch,filename):\n",
    "    if filename == 'metadata':\n",
    "        blobs = map(fix_json,batch)\n",
    "        df = DataFrame.from_records(blobs, columns=metadata_columns)\n",
    "    else:\n",
    "        blobs = map(to_json, batch)\n",
    "        df = DataFrame.from_records(blobs, columns=reviews_columns)\n",
    "    return df\n",
    "\n",
    "def create_new(path_to_file,df, current_index):\n",
    "    with h5py.File(path_to_file, \"w\") as hf:\n",
    "        hf.create_dataset('amazon', data=df,maxshape=(None,None),dtype=str_dt)\n",
    "        current_index+=chunksize\n",
    "    return hf, current_index\n",
    "\n",
    "def extend_hdf5(path_to_file,df, current_index, chunksize):\n",
    "    with h5py.File(path_to_file, \"r+\") as hf:\n",
    "        data = hf.get('amazon')\n",
    "        if len(df)<chunksize:\n",
    "            chunksize=len(df)\n",
    "        data.resize((data.shape[0]+chunksize,data.shape[1]))\n",
    "        data[current_index:current_index+chunksize] = df\n",
    "        current_index+=chunksize\n",
    "        return current_index\n",
    "\n",
    "def create_hdf5(fullpath,chunksize,current_index):\n",
    "    filename = fullpath.split('/')[-1].split('.')[0]\n",
    "    with open(fullpath,'r') as f:\n",
    "        batches = partition_all(chunksize, f)\n",
    "        store = None\n",
    "        for batch in batches:\n",
    "            df = to_df(batch,filename)\n",
    "            if store==None:\n",
    "                store, current_index = create_new(path_to_file,df,current_index)\n",
    "            else:\n",
    "                current_index = extend_hdf5(path_to_file,df, current_index, chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_hdf5(path_to+f,chunksize,current_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1384719342', 'A2IBPI20UZIR0U',\n",
       "        'cassandra tu \"Yeah, well, that\\'s just like, u...', ...,\n",
       "        \"Not much to write about here, but it does exactly what it's supposed to. filters out the pop sounds. now my recordings are much more crisp. it is one of the lowest prices pop filters on amazon so might as well buy it, they honestly work the same despite their pricing,\",\n",
       "        '02 28, 2014', '1393545600'],\n",
       "       ['1384719342', 'A14VAT5EAX3D9S', 'Jake', ...,\n",
       "        \"The product does exactly as it should and is quite affordable.I did not realized it was double screened until it arrived, so it was even better than I had expected.As an added bonus, one of the screens carries a small hint of the smell of an old grape candy I used to buy, so for reminiscent's sake, I cannot stop putting the pop filter next to my nose and smelling it after recording. :DIf you needed a pop filter, this will work just as well as the expensive ones, and it may even come with a pleasing aroma like mine did!Buy this product! :]\",\n",
       "        '03 16, 2013', '1363392000'],\n",
       "       ['1384719342', 'A195EZSQDW3E21', 'Rick Bennette \"Rick Bennette\"',\n",
       "        ...,\n",
       "        'The primary job of this device is to block the breath that would otherwise produce a popping sound, while allowing your voice to pass through with no noticeable reduction of volume or high frequencies. The double cloth filter blocks the pops and lets the voice through with no coloration. The metal clamp mount attaches to the mike stand secure enough to keep it attached. The goose neck needs a little coaxing to stay where you put it.',\n",
       "        '08 28, 2013', '1377648000'],\n",
       "       ..., \n",
       "       ['B00JBIVXGC', 'AWCJ12KBO5VII', 'Michael L. Knapp', ...,\n",
       "        \"I have tried coated strings in the past ( including Elixirs) and have never been very fond of them. Whenever I tried them I felt a certain disconnect from my guitar. Somewhat reminiscent of wearing condom. Not that I hated them, just didn't really love them. These are the best ones I've tried so far. I still don't like them as much as regular strings but because of the type of gigs I mostly do these seem to be a reasonable trade off. If you need a longer lasting string for whatever the reason these are really the best out there. After a dozen or so gigs with them, they still sound the same as when I put them on.\",\n",
       "        '07 22, 2014', '1405987200'],\n",
       "       ['B00JBIVXGC', 'A2Z7S8B5U4PAKJ', 'Rick Langdon \"Scriptor\"', ...,\n",
       "        \"Well, MADE by Elixir and DEVELOPED with Taylor Guitars ... these strings were designed for the new 800 (Rosewood) series guitars that came out this year (2014) ... the promise is a &#34;bolder high end, fuller low end&#34; ... I am a long-time Taylor owner and favor their 800 series (Rosewood/Spruce is my favorite combo in tone woods) ... I have almost always used Elixir Nanoweb Phosphor Bronze lights on my guitars ... I like not only the tone but the feel and longevity of these strings ... I have never had any issues with Elixir Nanowebs ... I recently picked up an 812ce First Edition 12-Fret ... such a fine instrument and it came with the Elixir HD's ... took some getting used to as far as feel (due to the slightly higher gauges of the treble strings - E, B & G) ... but as far as sound, they are great ... the D, A & low E strings are no different from the regular Elixir PB Lights so I am not sure about the claim of &#34;fuller low end&#34; ... compared to what?  Unless the extra string tension of the treble strings also contributes to a little more bass response ... I am not sure how these strings will perform on guitars other than Taylor's but what anyone should notice is more volume and clarity from the treble strings ... that is what I notice most from the HD's compared to the regular ... I still find no fault with the regular Elixir Nanaweb PB's but will most likely continue to run the HD's on my 12-fret ... I may also try them on my older 814ce just to see if there is any difference/improvement ... so far I find the set well balanced with good clarity and sustain ... try them out and make your own decision ...\",\n",
       "        '07 1, 2014', '1404172800'],\n",
       "       ['B00JBIVXGC', 'A2WA8TDCTGUADI', 'TheTerrorBeyond', ...,\n",
       "        \"These strings are really quite good, but I wouldn't call them perfect.  The unwound strings are not quite as bright as I am accustomed to, but they still ring nicely.  This is the only complaint I have about these strings.  If the unwound strings were a tiny bit brighter, these would be 5-star strings.  As it stands, I give them 4.5 stars... not a big knock, actually.The low-end on the wound strings is very nice and quite warm.  I put these on a jumbo and it definitely accentuates the &#34;jumbo&#34; aspect of my acoustic.  The sound is very big, full, and nice.Definitely a recommended product!4.5/5 stars\",\n",
       "        '07 16, 2014', '1405468800']], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = h5py.File(path_to_file)\n",
    "f['amazon'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda2/lib/python2.7/site-packages/tables/group.py:1213: UserWarning: problems loading leaf ``/amazon``::\n",
      "\n",
      "  variable length strings are not supported yet\n",
      "\n",
      "The leaf will become an ``UnImplemented`` node.\n",
      "  % (self._g_join(childname), exc))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input must be a list longer than 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c88982fefe3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mddf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/amazon'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/dataframe/io.pyc\u001b[0m in \u001b[0;36mread_hdf\u001b[0;34m(pattern, key, start, stop, columns, chunksize, lock)\u001b[0m\n\u001b[1;32m    576\u001b[0m                                     \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                                     lock=lock)\n\u001b[0;32m--> 578\u001b[0;31m                    for path in paths])\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/dataframe/io.pyc\u001b[0m in \u001b[0;36m_read_single_hdf\u001b[0;34m(path, key, start, stop, columns, chunksize, lock)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmulti\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     return concat([one_path_one_key(path, k, start, s, columns, chunksize, lock)\n\u001b[0;32m--> 515\u001b[0;31m                    for k, s in zip(keys, stops)])\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/dataframe/multi.pyc\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(dfs, axis, join, interleave_partitions)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input must be a list longer than 0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input must be a list longer than 0"
     ]
    }
   ],
   "source": [
    "ddf = dd.read_hdf(path_to_file,'/amazon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to = \"/home/ec2-user/amazon_dataset/\"\n",
    "path_to_file = \"/home/ec2-user/amazon_dataset/data.hdf5\"\n",
    "f = 'reviews_Musical_Instruments_5.json'\n",
    "reviews_columns = ['asin', 'reviewerID', 'overall','summary','reviewText','reviewTime','unixReviewTime']\n",
    "metadata_columns = ['asin','title','price','imUrl','related','also_bought','also_viewed','bought_together','salesRank','brand','categories']\n",
    "#str_dt = h5py.special_dtype(vlen=unicode)\n",
    "#dtypes = np.dtype([('asin',str_dt),('reviewerID',str_dt),('reviewerName',str_dt),('overall',float),('summary',str_dt),\n",
    " #                  ('reviewText',str_dt),('reviewTime',str_dt),('unixReviewTime',int)])\n",
    "chunksize = 5000\n",
    "current_index = 0\n",
    "\n",
    "#Convert a line of JSON into a cleaned up dict.\n",
    "def to_json(line):\n",
    "    return dissoc(ujson.loads(line.encode('utf8')),'reviewerName')\n",
    "\n",
    "#Convert a not proper line of JSON (due to single quotes) into a cleaned up dict.\n",
    "def fix_json(line):\n",
    "    return ast.literal_eval(line)\n",
    "\n",
    "#Convert a list of JSON strings into a DataFrame\n",
    "def to_df(batch,filename):\n",
    "    if filename == 'metadata':\n",
    "        blobs = map(fix_json,batch)\n",
    "        df = DataFrame.from_records(blobs, columns=metadata_columns)\n",
    "    else:\n",
    "        blobs = map(to_json, batch)\n",
    "        df = DataFrame.from_records(blobs, columns=reviews_columns)\n",
    "    return df\n",
    "\n",
    "def create_new(path_to_file,df, current_index):\n",
    "    types = df.apply(lambda x: pd.lib.infer_dtype(x.values))\n",
    "    for col in types[types=='unicode'].index:\n",
    "        df[col] = df[col].astype(str)\n",
    "    print df.apply(lambda x: pd.lib.infer_dtype(x.values))\n",
    "    df.to_hdf(path_to_file,'/amazon',format='table',data_columns=True,min_itemsize={'summary':150,'reviewText':12000})\n",
    "    current_index+=chunksize\n",
    "    return 'blabla', current_index\n",
    "\n",
    "def extend_hdf5(path_to_file,df, current_index, chunksize):\n",
    "    types = df.apply(lambda x: pd.lib.infer_dtype(x.values))\n",
    "    for col in types[types=='unicode'].index:\n",
    "        df[col] = df[col].astype(str)\n",
    "    df.to_hdf(path_to_file,'/amazon',format='table',data_columns=True,append=True)\n",
    "    current_index+=chunksize\n",
    "    return current_index\n",
    "\n",
    "def create_hdf5(fullpath,chunksize,current_index):\n",
    "    filename = fullpath.split('/')[-1].split('.')[0]\n",
    "    with open(fullpath,'r') as f:\n",
    "        batches = partition_all(chunksize, f)\n",
    "        store = None\n",
    "        for batch in batches:\n",
    "            df = to_df(batch,filename)\n",
    "            if store==None:\n",
    "                store, current_index = create_new(path_to_file,df,current_index)\n",
    "            else:\n",
    "                current_index = extend_hdf5(path_to_file,df, current_index, chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asin                string\n",
      "reviewerID          string\n",
      "overall           floating\n",
      "summary             string\n",
      "reviewText          string\n",
      "reviewTime          string\n",
      "unixReviewTime     integer\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda2/lib/python2.7/site-packages/tables/leaf.py:386: PerformanceWarning: The Leaf ``/amazon/_i_table/reviewText/sorted`` is exceeding the maximum recommended rowsize (104857600 bytes);\n",
      "be ready to see PyTables asking for *lots* of memory and possibly slow\n",
      "I/O.  You may want to reduce the rowsize by trimming the value of\n",
      "dimensions that are orthogonal (and preferably close) to the *main*\n",
      "dimension of this leave.  Alternatively, in case you have specified a\n",
      "very small/large chunksize, you may want to increase/decrease it.\n",
      "  PerformanceWarning)\n"
     ]
    }
   ],
   "source": [
    "create_hdf5(path_to+f,chunksize,current_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10261,)\n",
      "[ (0, '1384719342', 'A2IBPI20UZIR0U', 5.0, 'good', \"Not much to write about here, but it does exactly what it's supposed to. filters out the pop sounds. now my recordings are much more crisp. it is one of the lowest prices pop filters on amazon so might as well buy it, they honestly work the same despite their pricing,\", '02 28, 2014', 1393545600)\n",
      " (1, '1384719342', 'A14VAT5EAX3D9S', 5.0, 'Jake', \"The product does exactly as it should and is quite affordable.I did not realized it was double screened until it arrived, so it was even better than I had expected.As an added bonus, one of the screens carries a small hint of the smell of an old grape candy I used to buy, so for reminiscent's sake, I cannot stop putting the pop filter next to my nose and smelling it after recording. :DIf you needed a pop filter, this will work just as well as the expensive ones, and it may even come with a pleasing aroma like mine did!Buy this product! :]\", '03 16, 2013', 1363392000)\n",
      " (2, '1384719342', 'A195EZSQDW3E21', 5.0, 'It Does The Job Well', 'The primary job of this device is to block the breath that would otherwise produce a popping sound, while allowing your voice to pass through with no noticeable reduction of volume or high frequencies. The double cloth filter blocks the pops and lets the voice through with no coloration. The metal clamp mount attaches to the mike stand secure enough to keep it attached. The goose neck needs a little coaxing to stay where you put it.', '08 28, 2013', 1377648000)\n",
      " (3, '1384719342', 'A2C00NNG1ZQQG2', 5.0, 'GOOD WINDSCREEN FOR THE MONEY', 'Nice windscreen protects my MXL mic and prevents pops. Only thing is that the gooseneck is only marginally able to hold the screen in position and requires careful positioning of the clamp to avoid sagging.', '02 14, 2014', 1392336000)\n",
      " (4, '1384719342', 'A94QU4C90B1AX', 5.0, 'No more pops when I record my vocals.', \"This pop filter is great. It looks and performs like a studio filter. If you're recording vocals this will eliminate the pops that gets recorded when you sing.\", '02 21, 2014', 1392940800)\n",
      " (5, 'B00004Y2UT', 'A2A039TZMZHH9Y', 5.0, 'The Best Cable', 'So good that I bought another one.  Love the heavy cord and gold connectors.  Bass sounds great.  I just learned last night how to coil them up.  I guess I should read instructions more carefully.  But no harm done, still works great!', '12 21, 2012', 1356048000)\n",
      " (6, 'B00004Y2UT', 'A1UPZM995ZAH90', 5.0, \"Monster Standard 100 - 21' Instrument Cable\", \"I have used monster cables for years, and with good reason. The lifetime warranty is worth the price alone. Simple fact: cables break, but getting to replace them at no cost is where it's at.\", '01 19, 2014', 1390089600)\n",
      " (7, 'B00004Y2UT', 'AJNFQI3YR6XJ5', 3.0, \"Didn't fit my 1996 Fender Strat...\", \"I now use this cable to run from the output of my pedal chain to the input of my Fender Amp. After I bought Monster Cable to hook up my pedal board I thought I would try another one and update my guitar. I had been using a high end Planet Waves cable that I bought in the 1980's... Once I found out the input jacks on the new Monster cable didn't fit into the Fender Strat jack I was a little disappointed... I didn't return it and as stated I use it for the output on the pedal board. Save your money... I went back to my Planet Waves Cable...I payed $30.00 back in the eighties for the Planet Waves which now comes in at around $50.00. What I'm getting at is you get what you pay for. I thought Waves was a lot of money back in the day...but I haven't bought a guitar cable since this one...20 plus years and still working...Planet Waves wins.\", '11 16, 2012', 1353024000)\n",
      " (8, 'B00004Y2UT', 'A3M1PLEYNDEYO8', 5.0, 'Great cable', 'Perfect for my Epiphone Sheraton II.  Monster cables are well constructed.  I have several and never had any problems with any of them over the years.  Got this one because I wanted the 90 degree plug.', '07 6, 2008', 1215302400)\n",
      " (9, 'B00004Y2UT', 'AMNTZU1YQN1TH', 5.0, 'Best Instrument Cables On The Market', 'Monster makes the best cables and a lifetime warranty doesnt hurt either. This isnt their top of the line series but it works great with my bass guitar rig and has for some time. You cant go wrong with Monster Cables.', '01 8, 2014', 1389139200)]\n"
     ]
    }
   ],
   "source": [
    "#Check status\n",
    "with h5py.File(path_to_file, \"r+\") as hf:\n",
    "    data = hf.get('amazon/table')\n",
    "    print data.shape\n",
    "    print data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = dd.read_hdf(path_to_file,'/amazon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "\n\nTraceback\n---------\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/async.py\", line 267, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/async.py\", line 249, in _execute_task\n    return func(*args2)\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/dataframe/io.py\", line 523, in _pd_read_hdf\n    result = pd.read_hdf(path, key, **kwargs)\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/io/pytables.py\", line 360, in read_hdf\n    return store.select(key, auto_close=auto_close, **kwargs)\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/io/pytables.py\", line 724, in select\n    return it.get_result()\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/io/pytables.py\", line 1423, in get_result\n    results = self.func(self.start, self.stop, where)\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/io/pytables.py\", line 717, in func\n    columns=columns, **kwargs)\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/io/pytables.py\", line 4087, in read\n    if not self.read_axes(where=where, **kwargs):\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/io/pytables.py\", line 3288, in read_axes\n    values = self.selection.select()\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/io/pytables.py\", line 4686, in select\n    return self.table.table.read(start=self.start, stop=self.stop)\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/tables/table.py\", line 2008, in read\n    arr = self._read(start, stop, step, field, out)\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/tables/table.py\", line 1898, in _read\n    result = self._get_container(nrows)\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/tables/table.py\", line 1005, in _get_container\n    return numpy.empty(shape=shape, dtype=self._v_dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-119e5969cfa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'overall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'asin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'overall'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/base.pyc\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mExtra\u001b[0m \u001b[0mkeywords\u001b[0m \u001b[0mto\u001b[0m \u001b[0mforward\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \"\"\"\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/base.pyc\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mdsk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdask\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0mresults_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/threaded.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(dsk, result, cache, num_workers, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m     results = get_async(pool.apply_async, len(pool._pool), dsk, result,\n\u001b[1;32m     56\u001b[0m                         \u001b[0mcache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_thread_get_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/async.pyc\u001b[0m in \u001b[0;36mget_async\u001b[0;34m(apply_async, num_workers, dsk, result, cache, queue, get_id, raise_on_exception, rerun_exceptions_locally, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m                 \u001b[0m_execute_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Re-execute locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremote_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cache'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mfinish_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: \n\nTraceback\n---------\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/async.py\", line 267, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/async.py\", line 249, in _execute_task\n    return func(*args2)\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/dataframe/io.py\", line 523, in _pd_read_hdf\n    result = pd.read_hdf(path, key, **kwargs)\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/io/pytables.py\", line 360, in read_hdf\n    return store.select(key, auto_close=auto_close, **kwargs)\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/io/pytables.py\", line 724, in select\n    return it.get_result()\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/io/pytables.py\", line 1423, in get_result\n    results = self.func(self.start, self.stop, where)\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/io/pytables.py\", line 717, in func\n    columns=columns, **kwargs)\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/io/pytables.py\", line 4087, in read\n    if not self.read_axes(where=where, **kwargs):\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/io/pytables.py\", line 3288, in read_axes\n    values = self.selection.select()\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/io/pytables.py\", line 4686, in select\n    return self.table.table.read(start=self.start, stop=self.stop)\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/tables/table.py\", line 2008, in read\n    arr = self._read(start, stop, step, field, out)\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/tables/table.py\", line 1898, in _read\n    result = self._get_container(nrows)\n  File \"/home/ec2-user/anaconda2/lib/python2.7/site-packages/tables/table.py\", line 1005, in _get_container\n    return numpy.empty(shape=shape, dtype=self._v_dtype)\n"
     ]
    }
   ],
   "source": [
    "result = df.where(df['overall']<5).groupby('asin').mean().nlargest(10,columns='overall')\n",
    "result.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
