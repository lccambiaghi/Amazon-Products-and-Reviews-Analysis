{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask and Castra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This cell shows the fix for the `metadata.json` file:** basically, we use the library `ast` for properly encoding the JSON file, this means a slower encoding but anyway it works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 200000 300000 400000 500000 600000 700000 800000 900000 1000000 1000000\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import ast\n",
    "\n",
    "metadata_gz = \"/home/ec2-user/amazon_dataset/metadata.json.gz\"\n",
    "fivecore = '/home/ec2-user/amazon_dataset/kcore_5.json.gz'\n",
    "\n",
    "with gzip.open(metadata_gz, 'rb') as f:\n",
    "    for i,l in enumerate(f):\n",
    "        try:\n",
    "            k = ast.literal_eval(l)\n",
    "        except ValueError, e: \n",
    "            print '\\nERR\\n',e,'\\n' , l\n",
    "            break\n",
    "        if i % 1e5 == 0 and i>0:\n",
    "            print i,\n",
    "        \n",
    "        #Just not to process the whole file, we stop the computation after 1.000.000 lines\n",
    "        if i % 1e6 == 0 and i>0:\n",
    "            break\n",
    "\n",
    "print i\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the final version of the working implementation, step by step:\n",
    "\n",
    "All the **imports**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ujson\n",
    "import gzip\n",
    "import ast\n",
    "from pandas import DataFrame\n",
    "from toolz import dissoc\n",
    "from toolz import dissoc, partition_all\n",
    "from castra import Castra\n",
    "import time\n",
    "import datetime\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing variables like **paths**, **column names** and **chunk size**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to = \"/home/ec2-user/amazon_dataset/\"\n",
    "reviews = \"kcore_5.json.gz\"\n",
    "metadata = \"metadata.json.gz\"\n",
    "reviews_columns = ['asin', 'reviewerID','reviewerName', 'overall','summary','reviewText','reviewTime','unixReviewTime']\n",
    "metadata_columns = ['asin','title','price','imUrl','related','also_bought','also_viewed','bought_together','salesRank','brand','categories']\n",
    "chunksize = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the user defined **functions** we used in the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert a line of JSON into a cleaned up dict.\n",
    "def to_json(line):\n",
    "    return ujson.loads(line)\n",
    "\n",
    "#Convert a not proper line of JSON (due to single quotes) into a cleaned up dict.\n",
    "def fix_json(line):\n",
    "    return ast.literal_eval(line)\n",
    "\n",
    "#Convert a list of JSON strings into a DataFrame\n",
    "def to_df(batch,filename):\n",
    "    if filename == 'metadata':\n",
    "        blobs = map(fix_json,batch)\n",
    "        df = DataFrame.from_records(blobs, columns=metadata_columns)\n",
    "    else:\n",
    "        blobs = map(to_json, batch)\n",
    "        df = DataFrame.from_records(blobs, columns=reviews_columns)\n",
    "    return df\n",
    "\n",
    "#Create the castra dataset for improved I/O operations with Dask DataFrames\n",
    "#We can work properly on compressed GZ files with gzip library\n",
    "#The chunk size is 5000, which means that 5000 lines per time will be processed\n",
    "def create_castra(fullpath,chunksize):\n",
    "    filename = fullpath.split('/')[-1].split('.')[0]\n",
    "    with gzip.open(fullpath,'rb') as f:\n",
    "        batches = partition_all(chunksize, f)\n",
    "        castra = None\n",
    "        for batch in batches:\n",
    "            df = to_df(batch,filename)\n",
    "            if castra == None:\n",
    "                castra = Castra(path_to+filename+'.castra', template=df)\n",
    "            castra.extend(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT!!! DON'T RUN TWICE!** | **Execution of the script**: this may take a while... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the creation of the Castra files...\n",
      "Processing compressed metadata...\n",
      "Done! Metadata processed in: 0:29:01.749161\n"
     ]
    }
   ],
   "source": [
    "print 'Starting the creation of the Castra files...'\n",
    "\n",
    "#Creating the castra file for metadata\n",
    "print 'Processing compressed metadata...'\n",
    "start = time.time()\n",
    "create_castra(path_to+metadata,chunksize)\n",
    "end = time.time()\n",
    "print \"Done! Metadata processed in:\",datetime.timedelta(seconds=(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing compressed 5_cores...\n",
      "Done! Reviews data processed in: 0:26:57.887684\n"
     ]
    }
   ],
   "source": [
    "#Creating the castra file for the 5_cores\n",
    "print 'Processing compressed 5_cores...'\n",
    "start = time.time()\n",
    "create_castra(path_to+reviews,chunksize)\n",
    "end = time.time()\n",
    "print \"Done! Reviews data processed in:\",datetime.timedelta(seconds=(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the creation of the castra files, we can start playing with the **Dask DataFrames**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start a progress bar for all computations\n",
    "pbar = ProgressBar(minimum=3.0,dt=0.5)\n",
    "pbar.register()\n",
    "\n",
    "# Load data into a dask dataframe:\n",
    "path_to_castra = '/home/ec2-user/amazon_dataset/kcore_5.castra'\n",
    "df = dd.from_castra(path_to_castra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000013714</td>\n",
       "      <td>ACNGUPJ3A3TM9</td>\n",
       "      <td>GCM</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Nice Hymnal</td>\n",
       "      <td>We use this type of hymnal at church.  I was l...</td>\n",
       "      <td>12 3, 2013</td>\n",
       "      <td>1386028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000013714</td>\n",
       "      <td>A2SUAM1J3GNN3B</td>\n",
       "      <td>J. McDonald</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Heavenly Highway Hymns</td>\n",
       "      <td>I bought this for my husband who plays the pia...</td>\n",
       "      <td>09 13, 2009</td>\n",
       "      <td>1252800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000013714</td>\n",
       "      <td>APOZ15IEYQRRR</td>\n",
       "      <td>maewest64</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Awesome Hymn Book</td>\n",
       "      <td>This is a large size hymn book which is great ...</td>\n",
       "      <td>03 9, 2013</td>\n",
       "      <td>1362787200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000013714</td>\n",
       "      <td>AYEDW3BFK53XK</td>\n",
       "      <td>Missb</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Hand Clapping Toe Tapping Oldies</td>\n",
       "      <td>We use this hymn book at the mission.  It has ...</td>\n",
       "      <td>01 2, 2012</td>\n",
       "      <td>1325462400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000013714</td>\n",
       "      <td>A1KLCGLCXYP1U1</td>\n",
       "      <td>Paul L \"Paul Lytle\"</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Misleading</td>\n",
       "      <td>One review advised this book was large print, ...</td>\n",
       "      <td>08 10, 2013</td>\n",
       "      <td>1376092800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin      reviewerID         reviewerName  overall  \\\n",
       "0  0000013714   ACNGUPJ3A3TM9                  GCM      4.0   \n",
       "1  0000013714  A2SUAM1J3GNN3B          J. McDonald      5.0   \n",
       "2  0000013714   APOZ15IEYQRRR            maewest64      5.0   \n",
       "3  0000013714   AYEDW3BFK53XK                Missb      5.0   \n",
       "4  0000013714  A1KLCGLCXYP1U1  Paul L \"Paul Lytle\"      3.0   \n",
       "\n",
       "                            summary  \\\n",
       "0                       Nice Hymnal   \n",
       "1            Heavenly Highway Hymns   \n",
       "2                 Awesome Hymn Book   \n",
       "3  Hand Clapping Toe Tapping Oldies   \n",
       "4                        Misleading   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  We use this type of hymnal at church.  I was l...   12 3, 2013   \n",
       "1  I bought this for my husband who plays the pia...  09 13, 2009   \n",
       "2  This is a large size hymn book which is great ...   03 9, 2013   \n",
       "3  We use this hymn book at the mission.  It has ...   01 2, 2012   \n",
       "4  One review advised this book was large print, ...  08 10, 2013   \n",
       "\n",
       "   unixReviewTime  \n",
       "0      1386028800  \n",
       "1      1252800000  \n",
       "2      1362787200  \n",
       "3      1325462400  \n",
       "4      1376092800  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 10min 49.6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "asin              41135700\n",
       "overall           41135700\n",
       "reviewText        41135700\n",
       "reviewTime        41135700\n",
       "reviewerID        41135700\n",
       "reviewerName      40203149\n",
       "summary           41135700\n",
       "unixReviewTime    41135696\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  1min 28.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "B00FAPF5U0    13550\n",
       "B0051VVOB2    11981\n",
       "B0074BW614    10836\n",
       "030758836X    10552\n",
       "0439023483    10404\n",
       "B00DR0PDNE    10139\n",
       "B007WTAJTO     9771\n",
       "B006GWO5WK     9008\n",
       "B005SUHPO6     8963\n",
       "B0064X7B4A     8808\n",
       "Name: asin, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.asin.value_counts().nlargest(10).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying just with Dask importing the `.json` file without dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  2min 27.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8898041"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js = db.read_text(\"/home/ec2-user/amazon_dataset/reviews_Books_5.json\").map(ujson.loads)\n",
    "js.count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
