{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First example with HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import ujson\n",
    "import gzip\n",
    "import ast\n",
    "from pandas import DataFrame\n",
    "from toolz import dissoc\n",
    "from toolz import dissoc, partition_all\n",
    "from castra import Castra\n",
    "import time\n",
    "import datetime\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "from dask.diagnostics import ProgressBar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import HDFStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_to_file = \"/home/ec2-user/amazon_dataset/data.hdf5\"\n",
    "df = pd.DataFrame(np.random.randn(10, 5))\n",
    "current_index = 10 #Starting from 0, pointing to the first \"free\" position\n",
    "chunks = 10 #writing 10 lines each time\n",
    "\n",
    "with h5py.File(path_to_file, \"w\") as hf:\n",
    "    hf.create_dataset('amazon', data=df,maxshape=(None,None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of arrays in this file: \n",
      "[u'amazon']\n",
      "Shape of the array amazon: \n",
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path_to_file, \"r\") as hf:\n",
    "    print 'List of arrays in this file: \\n', hf.keys()\n",
    "    data = hf.get('amazon')\n",
    "    np_data = np.array(data)\n",
    "    print'Shape of the array amazon: \\n', np_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to reshape and add more content to the HDF5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of arrays in this file: \n",
      "[u'amazon']\n",
      "Hierarchy: /amazon\n",
      "Shape of the array amazon: \n",
      "(20, 5)\n",
      "Data:\n",
      "[[ 0.33615614  0.92148885  0.80791913 -0.07501982  1.50430268]\n",
      " [ 0.7479168   0.65582071 -0.71301003  0.9000312   2.12067986]\n",
      " [ 0.28266367 -0.93705573 -0.93001321 -0.04424039  0.37498966]\n",
      " [ 0.69727914  0.88918464 -2.12194092  1.03283473 -0.23717406]\n",
      " [-0.22126272 -0.18595924  0.70276079 -0.25828817  0.03022614]\n",
      " [-0.25548966 -0.50407185 -0.68975554  0.67881699  0.375861  ]\n",
      " [ 1.68238172 -0.94102896 -0.91619115 -0.22347315 -0.48222473]\n",
      " [ 0.44037292  0.1177223   0.73702812 -0.71398744  1.0187634 ]\n",
      " [-0.99370112 -0.366224   -0.56279895  0.26389202 -0.59253603]\n",
      " [-0.71683573  1.2921071  -1.59845757  0.719606   -0.8612614 ]\n",
      " [ 0.72463843  0.07863037  0.00382157 -1.39810272 -1.81331261]\n",
      " [-0.45035738  0.7919192  -0.40131715  0.12347294 -0.81692247]\n",
      " [ 0.07909288  1.13137146  1.43555496 -1.92644076  1.05954108]\n",
      " [ 0.68715044 -0.55307139 -1.49701966 -1.61766041  0.59391732]\n",
      " [-2.79255144 -0.8156708   0.14451914  0.55447627 -0.69890438]\n",
      " [ 1.15234974 -1.39836278 -0.69343631 -0.70537229 -1.03612067]\n",
      " [-0.29628214 -0.20714572 -1.00386495 -3.06484895  0.77176389]\n",
      " [ 0.41928378  0.27947715 -0.53829255  0.39573861  1.95734273]\n",
      " [-1.12169001 -0.17934515 -1.15341126 -0.05267281  0.47773748]\n",
      " [-0.38232652  0.86209742  0.31018013 -1.11466294  0.08811476]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path_to_file, \"r+\") as hf:\n",
    "    data = hf.get('amazon')\n",
    "    df = pd.DataFrame(np.random.randn(10, 5)) #New dataframe to add\n",
    "    data.resize((data.shape[0]+chunks,data.shape[1]))\n",
    "    data[current_index:current_index+chunks] = df\n",
    "    print 'List of arrays in this file: \\n', hf.keys()\n",
    "    print 'Hierarchy:',data.name\n",
    "    np_data = np.array(data)\n",
    "    print'Shape of the array amazon: \\n', np_data.shape\n",
    "    print 'Data:\\n',data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input must be a list longer than 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-126cacf47380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/amazon'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/dataframe/io/hdf.pyc\u001b[0m in \u001b[0;36mread_hdf\u001b[0;34m(pattern, key, start, stop, columns, chunksize, sorted_index, lock, mode)\u001b[0m\n\u001b[1;32m    316\u001b[0m                                     \u001b[0msorted_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorted_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                                     lock=lock, mode=mode)\n\u001b[0;32m--> 318\u001b[0;31m                    for path in paths])\n\u001b[0m",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/dataframe/io/hdf.pyc\u001b[0m in \u001b[0;36m_read_single_hdf\u001b[0;34m(path, key, start, stop, columns, chunksize, sorted_index, lock, mode)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     return concat([one_path_one_key(path, k, start, s, columns, chunksize, d, lock)\n\u001b[0;32m--> 248\u001b[0;31m                    for k, s, d in zip(keys, stops, divisions)])\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/dataframe/multi.pyc\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(dfs, axis, join, interleave_partitions)\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input must be a list longer than 0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input must be a list longer than 0"
     ]
    }
   ],
   "source": [
    "dd.read_hdf(path_to_file,'/amazon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now creating the Dask DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,)\n",
      "[ (0, [0.897718067236105, -1.0946661914237923, 2.0314060744354916, 0.37843178259651294, 1.8832041047888866])\n",
      " (1, [-0.28564043136680733, -0.07655075474193072, 0.540224405128584, -0.35796842299738124, -0.9406625980997961])\n",
      " (2, [0.3205245813708483, -0.7609832345599711, -0.5724229802992692, 0.7995682782984899, 0.39250741780216425])\n",
      " (3, [0.9428039707471949, 0.702020781965266, -0.05146169323140638, 0.8946086778295255, 0.37578254621924284])\n",
      " (4, [-0.23418750846014688, 0.7638746474643954, -1.1318622671203682, 1.5617540487978496, -0.6106827553232297])\n",
      " (5, [-0.4677923345600671, 1.1000987559518314, 0.38678789045171685, -0.93770514543715, 0.18821073801108623])\n",
      " (6, [2.188181894477686, -0.30098460344717487, 0.13535327088522206, 1.2107808149056434, 0.5053363211870221])\n",
      " (7, [-0.8534687441165866, 1.3112811679624061, -1.0672488905501138, 0.8436103332715101, -0.23720971404964686])\n",
      " (8, [-0.014267216760865736, -2.439805782094342, -0.16067586652974358, 0.2510792452943708, -0.833267244505457])\n",
      " (9, [-0.10374909948924228, 0.1867006970075715, -0.8209544692240448, 0.18147671039731245, -1.2786671247199302])\n",
      " (0, [-1.2046634836086856, 0.0384071085258002, -1.7848153871268664, -1.9666081505961863, 1.722756363113334])\n",
      " (1, [-0.056418982247454216, -1.8285338533231072, -0.12532139429428138, 1.186552928241772, 0.9509844289148641])\n",
      " (2, [-0.02809093643112384, -0.9788303079674272, 0.9790442267131813, 0.6365353366441363, -0.49379977114187135])\n",
      " (3, [0.45860997219293054, -0.5016746743752692, 0.9261741071486651, -0.42167995979879996, -2.0422905045038644])\n",
      " (4, [1.347567104488467, 0.8813140721527314, -0.9856505093655008, 1.4651241828516235, -1.6262396192762896])\n",
      " (5, [-0.5113633629825338, -0.28406330154438547, 2.1060004724162495, -0.7927183474751764, -0.33430990736819577])\n",
      " (6, [0.23117627025532939, -0.8410266112813619, 0.7845815416698819, 1.8945153095025165, -0.27910217528315573])\n",
      " (7, [0.14121875482441998, -0.07562923918455881, -0.9137324061885284, 0.45953723475035274, -2.006930035682716])\n",
      " (8, [-0.5369511740829129, 0.8001258188404748, -0.07576742964934707, -0.1009706385003627, -0.5965725346683902])\n",
      " (9, [0.8820238242655756, 1.4883891118767245, 0.23542093519155785, 0.7864400452570262, -1.1683777846350119])\n",
      " (0, [-1.4054462187534063, -0.08016304043859487, 0.38113440132390897, -0.33118749120850727, 1.4753945000163844])\n",
      " (1, [-1.5042729128929604, -0.8272850161635443, -1.4262924791457985, 1.7699178833086802, 0.29238226875964746])\n",
      " (2, [0.2774431589708314, 1.1492076112143386, 0.5041922697064373, 0.5999060748938959, 0.2902979991313325])\n",
      " (3, [0.2819276947404195, 0.7816267262858749, 1.5751577598507165, 0.16446607887392686, -0.9145353302423488])\n",
      " (4, [-1.145905844266239, 1.9433432467182536, -1.3055303405739698, -2.1412261694203716, 0.23908443470666607])\n",
      " (5, [1.955002851737706, 0.11135805358228863, -2.163489659106752, 0.46069292561677977, -2.3740854278175823])\n",
      " (6, [0.9098817606780468, 0.29099394743615264, 0.48492099357543006, 0.2879361368634146, -1.0290563347258237])\n",
      " (7, [-0.07473516847041223, -0.5790234224720945, -1.6108268670687045, -0.3789683014722695, 0.5989803982602618])\n",
      " (8, [1.32308030264701, -1.0410774971485368, -0.10071350512545736, 0.8378335989655715, -0.6914159663872786])\n",
      " (9, [0.18307028792105046, -0.5329858682459945, 0.8705711471029638, -0.3621846772676795, 0.26921189435321274])]\n"
     ]
    }
   ],
   "source": [
    "#First DF\n",
    "df = pd.DataFrame(np.random.randn(10, 5))\n",
    "#Create HDF5\n",
    "df.to_hdf(path_to_file,'/amazon',format='table')\n",
    "\n",
    "#Second DF\n",
    "df = pd.DataFrame(np.random.randn(10, 5))\n",
    "df.to_hdf(path_to_file,'/amazon',format='table',append=True)\n",
    "\n",
    "#Third DF\n",
    "df = pd.DataFrame(np.random.randn(10, 5))\n",
    "df.to_hdf(path_to_file,'/amazon',format='table',append=True)\n",
    "\n",
    "#Check status\n",
    "with h5py.File(path_to_file, \"r+\") as hf:\n",
    "    data = hf.get('amazon/table')\n",
    "    print data.shape\n",
    "    print data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ (0, [0.8431498700118035, -0.26071150407298455, -0.17717400333493152, 0.972291626015662, -1.9821576100992873])\n",
      " (1, [0.25442721137162405, -0.9508859044686622, -0.6912219066515594, 1.696716124429185, -0.12900855920293555])\n",
      " (2, [0.7609659073693733, -0.1636926592643042, 1.5539749829369545, 1.6315731234975972, -0.7995324229487413])\n",
      " (3, [-0.27556138904929234, 0.05216452252019807, -3.3079324342059646, -0.46611052296224664, 1.2176568561745245])\n",
      " (4, [-1.6228427558698009, -0.5702520598240428, 0.49270022069764885, 0.7664246954510092, -1.5151507755045541])\n",
      " (5, [-0.47711779501545337, -1.768899129071236, -0.21830344536814678, 2.0949026925302703, -0.5553597531906278])\n",
      " (6, [-0.9539068208163176, -2.2245187760419403, -0.8068229606373949, 1.1477310920064447, -1.5644619776974054])\n",
      " (7, [-1.0099870383961342, 0.8550006144060289, 0.4543298642506465, 0.3700122002012492, 1.2046136946214707])\n",
      " (8, [-0.29723422304075203, 0.2995589052986324, -1.2861135643338708, -1.8224759858498658, -0.5396499969987226])\n",
      " (9, [0.7885687803260464, -2.28301442791481, -0.48417486440438984, 0.6625706535815433, -0.6696211630719618])]\n",
      "(10,)\n",
      "(20,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Can't broadcast (10, 5) -> (10,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-771ebbb547e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcurrent_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Data:\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/work/h5py/_objects.c:2696)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/work/h5py/_objects.c:2654)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/h5py/_hl/dataset.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, args, val)\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0mmshape_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mmspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNLIMITED\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfspace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/h5py/_hl/selections.pyc\u001b[0m in \u001b[0;36mbroadcast\u001b[0;34m(self, target_shape)\u001b[0m\n\u001b[1;32m    295\u001b[0m                     \u001b[0mtshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't broadcast %s -> %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0mtshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mtshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't broadcast (10, 5) -> (10,)"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.randn(10, 5))\n",
    "df.to_hdf(path_to_file,'/amazon',format='table')\n",
    "ddf = dd.read_hdf(path_to_file,key='/amazon')\n",
    "with h5py.File(path_to_file, \"r+\") as hf:\n",
    "    data = hf.get('amazon/table')\n",
    "    print data[:]\n",
    "    df = pd.DataFrame(np.random.randn(10, 5))\n",
    "    print data.shape\n",
    "    data.resize((data.shape[0]+chunks,))\n",
    "    print data.shape\n",
    "    data[current_index:current_index+chunks] = df\n",
    "    print 'Data:\\n',data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.526320</td>\n",
       "      <td>-0.085739</td>\n",
       "      <td>-1.037743</td>\n",
       "      <td>1.025857</td>\n",
       "      <td>0.873915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.235845</td>\n",
       "      <td>0.784827</td>\n",
       "      <td>-0.932460</td>\n",
       "      <td>-0.672638</td>\n",
       "      <td>-1.338503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.446893</td>\n",
       "      <td>0.084582</td>\n",
       "      <td>0.160157</td>\n",
       "      <td>-0.445192</td>\n",
       "      <td>-1.402381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.184008</td>\n",
       "      <td>-0.193218</td>\n",
       "      <td>-0.272635</td>\n",
       "      <td>-1.414573</td>\n",
       "      <td>0.524036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.961818</td>\n",
       "      <td>1.926870</td>\n",
       "      <td>0.943832</td>\n",
       "      <td>0.347053</td>\n",
       "      <td>0.923796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.791976</td>\n",
       "      <td>-0.918043</td>\n",
       "      <td>0.300772</td>\n",
       "      <td>0.310206</td>\n",
       "      <td>-0.238695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.528471</td>\n",
       "      <td>0.684842</td>\n",
       "      <td>0.568258</td>\n",
       "      <td>0.786830</td>\n",
       "      <td>-0.107109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.252557</td>\n",
       "      <td>0.377502</td>\n",
       "      <td>1.178489</td>\n",
       "      <td>0.920960</td>\n",
       "      <td>-0.804955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.152096</td>\n",
       "      <td>-1.653841</td>\n",
       "      <td>2.309369</td>\n",
       "      <td>-2.517307</td>\n",
       "      <td>-1.423715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.681187</td>\n",
       "      <td>-2.534503</td>\n",
       "      <td>1.879892</td>\n",
       "      <td>1.621802</td>\n",
       "      <td>-1.031088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4\n",
       "0 -0.526320 -0.085739 -1.037743  1.025857  0.873915\n",
       "1 -0.235845  0.784827 -0.932460 -0.672638 -1.338503\n",
       "2  0.446893  0.084582  0.160157 -0.445192 -1.402381\n",
       "3  0.184008 -0.193218 -0.272635 -1.414573  0.524036\n",
       "4 -0.961818  1.926870  0.943832  0.347053  0.923796\n",
       "5 -0.791976 -0.918043  0.300772  0.310206 -0.238695\n",
       "6  0.528471  0.684842  0.568258  0.786830 -0.107109\n",
       "7 -1.252557  0.377502  1.178489  0.920960 -0.804955\n",
       "8  1.152096 -1.653841  2.309369 -2.517307 -1.423715\n",
       "9  0.681187 -2.534503  1.879892  1.621802 -1.031088\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DataFrames with data from JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_to = \"/home/ec2-user/amazon_dataset/\"\n",
    "f = 'reviews_Musical_Instruments_5.json'\n",
    "reviews_columns = ['asin', 'reviewerID','reviewerName', 'overall','summary','reviewText','reviewTime','unixReviewTime']\n",
    "metadata_columns = ['asin','title','price','imUrl','related','also_bought','also_viewed','bought_together','salesRank','brand','categories']\n",
    "str_dt = h5py.special_dtype(vlen=str)\n",
    "#dtypes = np.dtype([('asin',str_dt),('reviewerID',str_dt),('reviewerName',str_dt),('overall',float),('summary',str_dt),\n",
    " #                  ('reviewText',str_dt),('reviewTime',str_dt),('unixReviewTime',int)])\n",
    "chunksize = 5000\n",
    "current_index = 0\n",
    "\n",
    "#Convert a line of JSON into a cleaned up dict.\n",
    "def to_json(line):\n",
    "    return ujson.loads(line.encode('utf8'))\n",
    "\n",
    "#Convert a not proper line of JSON (due to single quotes) into a cleaned up dict.\n",
    "def fix_json(line):\n",
    "    return ast.literal_eval(line)\n",
    "\n",
    "#Convert a list of JSON strings into a DataFrame\n",
    "def to_df(batch,filename):\n",
    "    if filename == 'metadata':\n",
    "        blobs = map(fix_json,batch)\n",
    "        df = DataFrame.from_records(blobs, columns=metadata_columns)\n",
    "    else:\n",
    "        blobs = map(to_json, batch)\n",
    "        df = DataFrame.from_records(blobs, columns=reviews_columns)\n",
    "    return df\n",
    "\n",
    "def create_new(path_to_file,df, current_index):\n",
    "    with h5py.File(path_to_file, \"w\") as hf:\n",
    "        hf.create_dataset('amazon', data=df,maxshape=(None,None),dtype=str_dt)\n",
    "        current_index+=chunksize\n",
    "    return hf, current_index\n",
    "\n",
    "def extend_hdf5(path_to_file,df, current_index, chunksize):\n",
    "    with h5py.File(path_to_file, \"r+\") as hf:\n",
    "        data = hf.get('amazon')\n",
    "        if len(df)<chunksize:\n",
    "            chunksize=len(df)\n",
    "        data.resize((data.shape[0]+chunksize,data.shape[1]))\n",
    "        data[current_index:current_index+chunksize] = df\n",
    "        current_index+=chunksize\n",
    "        return current_index\n",
    "\n",
    "def create_hdf5(fullpath,chunksize,current_index):\n",
    "    filename = fullpath.split('/')[-1].split('.')[0]\n",
    "    with open(fullpath,'r') as f:\n",
    "        batches = partition_all(chunksize, f)\n",
    "        store = None\n",
    "        for batch in batches:\n",
    "            df = to_df(batch,filename)\n",
    "            if store==None:\n",
    "                store, current_index = create_new(path_to_file,df,current_index)\n",
    "            else:\n",
    "                current_index = extend_hdf5(path_to_file,df, current_index, chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_hdf5(path_to+f,chunksize,current_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input must be a list longer than 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-c88982fefe3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mddf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/amazon'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/dataframe/io/hdf.pyc\u001b[0m in \u001b[0;36mread_hdf\u001b[0;34m(pattern, key, start, stop, columns, chunksize, sorted_index, lock, mode)\u001b[0m\n\u001b[1;32m    316\u001b[0m                                     \u001b[0msorted_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorted_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                                     lock=lock, mode=mode)\n\u001b[0;32m--> 318\u001b[0;31m                    for path in paths])\n\u001b[0m",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/dataframe/io/hdf.pyc\u001b[0m in \u001b[0;36m_read_single_hdf\u001b[0;34m(path, key, start, stop, columns, chunksize, sorted_index, lock, mode)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     return concat([one_path_one_key(path, k, start, s, columns, chunksize, d, lock)\n\u001b[0;32m--> 248\u001b[0;31m                    for k, s, d in zip(keys, stops, divisions)])\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/dask/dataframe/multi.pyc\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(dfs, axis, join, interleave_partitions)\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input must be a list longer than 0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input must be a list longer than 0"
     ]
    }
   ],
   "source": [
    "ddf = dd.read_hdf(path_to_file,'/amazon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to = \"/home/ec2-user/amazon_dataset/\"\n",
    "f = 'reviews_Musical_Instruments_5.json'\n",
    "reviews_columns = ['asin', 'reviewerID','reviewerName', 'overall','summary','reviewText','reviewTime','unixReviewTime']\n",
    "metadata_columns = ['asin','title','price','imUrl','related','also_bought','also_viewed','bought_together','salesRank','brand','categories']\n",
    "str_dt = h5py.special_dtype(vlen=unicode)\n",
    "#dtypes = np.dtype([('asin',str_dt),('reviewerID',str_dt),('reviewerName',str_dt),('overall',float),('summary',str_dt),\n",
    " #                  ('reviewText',str_dt),('reviewTime',str_dt),('unixReviewTime',int)])\n",
    "chunksize = 5000\n",
    "current_index = 0\n",
    "\n",
    "#Convert a line of JSON into a cleaned up dict.\n",
    "def to_json(line):\n",
    "    return dissoc(ujson.loads(line.encode('utf8')),'reviewerName')\n",
    "\n",
    "#Convert a not proper line of JSON (due to single quotes) into a cleaned up dict.\n",
    "def fix_json(line):\n",
    "    return ast.literal_eval(line)\n",
    "\n",
    "#Convert a list of JSON strings into a DataFrame\n",
    "def to_df(batch,filename):\n",
    "    if filename == 'metadata':\n",
    "        blobs = map(fix_json,batch)\n",
    "        df = DataFrame.from_records(blobs, columns=metadata_columns)\n",
    "    else:\n",
    "        blobs = map(to_json, batch)\n",
    "        df = DataFrame.from_records(blobs, columns=reviews_columns)\n",
    "    return df\n",
    "\n",
    "def create_new(path_to_file,df, current_index):\n",
    "    types = df.apply(lambda x: pd.lib.infer_dtype(x.values))\n",
    "    for col in types[types=='unicode'].index:\n",
    "        df[col] = df[col].astype(str)\n",
    "    df.columns = [str(c) for c in df.columns]\n",
    "    df.to_hdf(path_to_file,'/amazon',format='table',data_columns=True)\n",
    "    current_index+=chunksize\n",
    "    return hf, current_index\n",
    "\n",
    "def extend_hdf5(path_to_file,df, current_index, chunksize):\n",
    "    df.to_hdf(path_to_file,'/amazon',format='table',data_columns=True,append=True)\n",
    "    current_index+=chunksize\n",
    "    return current_index\n",
    "\n",
    "def create_hdf5(fullpath,chunksize,current_index):\n",
    "    filename = fullpath.split('/')[-1].split('.')[0]\n",
    "    with open(fullpath,'r') as f:\n",
    "        batches = partition_all(chunksize, f)\n",
    "        store = None\n",
    "        for batch in batches:\n",
    "            df = to_df(batch,filename)\n",
    "            if store==None:\n",
    "                store, current_index = create_new(path_to_file,df,current_index)\n",
    "            else:\n",
    "                current_index = extend_hdf5(path_to_file,df, current_index, chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda2/lib/python2.7/site-packages/tables/leaf.py:386: PerformanceWarning: The Leaf ``/amazon/_i_table/reviewText/sorted`` is exceeding the maximum recommended rowsize (104857600 bytes);\n",
      "be ready to see PyTables asking for *lots* of memory and possibly slow\n",
      "I/O.  You may want to reduce the rowsize by trimming the value of\n",
      "dimensions that are orthogonal (and preferably close) to the *main*\n",
      "dimension of this leave.  Alternatively, in case you have specified a\n",
      "very small/large chunksize, you may want to increase/decrease it.\n",
      "  PerformanceWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "[unicode] is not implemented as a table column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-b65e5a94b949>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Check status\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'amazon/table'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-179-f82f1052c4a5>\u001b[0m in \u001b[0;36mcreate_hdf5\u001b[0;34m(fullpath, chunksize, current_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mcurrent_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-179-f82f1052c4a5>\u001b[0m in \u001b[0;36mextend_hdf5\u001b[0;34m(path_to_file, df, current_index, chunksize)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextend_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/amazon'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'table'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mcurrent_index\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcurrent_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mto_hdf\u001b[0;34m(self, path_or_buf, key, **kwargs)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpytables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_msgpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/io/pytables.pyc\u001b[0m in \u001b[0;36mto_hdf\u001b[0;34m(path_or_buf, key, value, mode, complevel, complib, append, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         with HDFStore(path_or_buf, mode=mode, complevel=complevel,\n\u001b[1;32m    269\u001b[0m                       complib=complib) as store:\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/io/pytables.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(store)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/io/pytables.pyc\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, key, value, format, append, columns, dropna, **kwargs)\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m         self._write_to_group(key, value, append=append, dropna=dropna,\n\u001b[0;32m--> 970\u001b[0;31m                              **kwargs)\n\u001b[0m\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m     def append_to_multiple(self, d, value, selector, data_columns=None,\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/io/pytables.pyc\u001b[0m in \u001b[0;36m_write_to_group\u001b[0;34m(self, key, value, format, index, append, complib, encoding, **kwargs)\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0;31m# write the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplib\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomplib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_table\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/io/pytables.pyc\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, obj, axes, append, complib, complevel, fletcher32, min_itemsize, chunksize, expectedrows, dropna, **kwargs)\u001b[0m\n\u001b[1;32m   3851\u001b[0m         self.create_axes(axes=axes, obj=obj, validate=append,\n\u001b[1;32m   3852\u001b[0m                          \u001b[0mmin_itemsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_itemsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3853\u001b[0;31m                          **kwargs)\n\u001b[0m\u001b[1;32m   3854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3855\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda2/lib/python2.7/site-packages/pandas/io/pytables.pyc\u001b[0m in \u001b[0;36mcreate_axes\u001b[0;34m(self, axes, obj, validate, nan_rep, data_columns, min_itemsize, **kwargs)\u001b[0m\n\u001b[1;32m   3523\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues_axes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3524\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNotImplementedError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3525\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdetail\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m                 raise Exception(\n",
      "\u001b[0;31mTypeError\u001b[0m: [unicode] is not implemented as a table column"
     ]
    }
   ],
   "source": [
    "create_hdf5(path_to+f,chunksize,current_index)\n",
    "\n",
    "#Check status\n",
    "with h5py.File(path_to_file, \"r+\") as hf:\n",
    "    data = hf.get('amazon/table')\n",
    "    print data.shape\n",
    "    print data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x    object\n",
       "y    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([['a','1'],['b','2']], columns=['x','y'])\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:1: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "x    object\n",
       "y     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.convert_objects(convert_dates=True,convert_numeric=True,convert_timedeltas=True).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([['a','1'],['b','2']], columns=['x','y'])\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
